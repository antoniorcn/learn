{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d8b8f3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../dados/portuguese/compliment.yml\n",
      "../../dados/portuguese/conversations.yml\n",
      "../../dados/portuguese/games.yml\n",
      "../../dados/portuguese/greetings.yml\n",
      "../../dados/portuguese/linguistic_knowledge.yml\n",
      "../../dados/portuguese/money.yml\n",
      "../../dados/portuguese/proverbs.yml\n",
      "../../dados/portuguese/suggestions.yml\n",
      "../../dados/portuguese/trivia.yml\n",
      "../../dados/portuguese/unilab.yml\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers , activations , models , preprocessing, utils\n",
    "import re\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../../dados/portuguese/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea770fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "dir_path = '../../dados/portuguese/'\n",
    "files_list = os.listdir(dir_path + os.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab739a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "[['você é linda', 'eu sei'], ['adoro você', 'também'], ['gosto da sua sinceridade', 'que bom'], ['você é valente', 'obrigado'], ['você é muito sexy', 'você é que é'], ['gosto do seu talento', 'que bom'], ['você é de mais', 'sim, eu sei'], ['você é uma graça', 'nem tanto'], ['como você é animada', 'é bom para saúde ser animada'], ['tu és insubstituível', 'ninguém é insubstituível'], ['tu trouxeste luz a minha vida', 'você é que trouxe luz a minha vida'], ['eu amo os teus expressivos olhos doces', 'eu te amo por completo'], ['estou muito orgulhoso de ti', 'que bom, obrigada'], [' estou muito orgulhoso de você', 'obrigada'], ['estou muito orgulhosa de ti', 'que bom, obrigado'], ['estou muito orgulhosa de você', 'obrigado'], ['você é tão engraçado', 'você acha?'], ['você é ótimo na cama', 'que bom'], ['você é ótima na cama', 'que bom'], ['você está linda', 'obrigada'], ['você está lindo', 'você é que está'], ['você é divertido', 'obrigado'], ['você é divertida', 'obrigada'], ['você é tão inteligente', 'estudo para isso'], ['você tem bom gosto', 'é sim'], ['tu és maravilhosa', 'você também'], ['tu és maravilhoso', 'você também é maravilhosa'], ['que pernas lindas você tem', 'obrigada'], ['você fala muito bem', 'você também fala bem'], ['você tem talento', 'tenho sim'], ['gosto do seu comportamento', 'obrigado'], [' gosto do seu comportamento', 'obrigada'], ['teu olhar me seduz', 'que bom'], ['você está mais bonita hoje', 'são seus olhos'], ['você está mais bonito hoje', 'obrigado'], ['tu estas mais bem arrumado hoje', 'você também'], ['seus olhos brilham como as estrelas', 'é porque te vi'], ['admiro-te por seu talento', 'você também é talentoso'], ['você está linda nesse vestido', 'obrigada'], ['você me faz melhor', 'nosso amor é que te faz melhor'], ['tuas palavras me confortam', 'sinto-me feliz por ouvir isso de você'], ['você é show', 'você é que é'], ['com essa beleza toda você me deixa nas nuvens', 'obrigada'], ['você é show de bola', 'você também'], ['você é um anjo que apareceu em minha vida', 'você é que é meu anjo'], ['você é fofa', 'você também é fofo'], ['tu és minha alma gêmea', 'você também é a minha alma gêmea'], ['gosto de você', 'também gosto de ti'], ['seu amor me fascina', 'que ótimo também sou fascinada pelo seu amor'], ['tu és a flor que faltava no meu jardim', 'você é o jardim que eu procurava'], ['você é a mulher da minha vida', 'Que bom!'], ['teu sorriso é como um jardim florido', 'isso só é possível por causa do teu amor'], ['tudo em você é maravilhoso', 'e em você é mais que maravilhoso, é sensacional'], ['você é luz para os meus olhos', 'tu és colírio para os meus'], ['tua pele é macia como uma lã', 'é para você amacia-la'], ['tua verdade é o que me atrai', 'gosto de ser verdadeira'], ['tu és meiga como uma rosa', 'são seus olhos que me veem assim'], ['seus atos de sinceridade me fascinam', 'espero ser sempre assim, fascinante'], ['teu perfume me atrai', 'nem só você é atraído por ele'], ['tu tens um corpo maravilhoso', 'o teu corpo também é atraente'], ['você é muito gostosa', 'como você sabe, você nunca provou dele'], ['teus lábios tem o gosto de mel', 'os teus são atraentes demais']]\n",
      "********************************************************************************\n",
      "[['Bom Dia como você está?', 'Eu estou bem, e você?', 'Eu também estou.', 'Que bom.', 'Sim.'], ['Olá', 'Oi', 'Como vai você?', 'Eu estou bem.', 'Que bom', 'Sim.', 'Posso ajudá-lo com alguma coisa?', 'Sim, eu tenho uma pergunta.', 'Qual é a sua pergunta?', 'Eu poderia pedir uma xícara de açúcar?', 'Me desculpe, mas eu não tenho nenhum.', 'Obrigado de qualquer maneira', 'Sem problemas'], ['Como vai você?', 'Eu estou bem, e você?', 'Eu também estou bem.', 'Isso é bom.'], ['Ouviu as notícias?', 'Que boa notícia?'], ['Qual é o seu livro favorito?', 'Eu não sei ler.', 'Então, qual é a sua cor favorita?', 'Azul'], ['Quem é você?', 'Quem? Quem é senão uma forma seguindo a função de quê', 'Então o que você é?', 'Um homem em uma máscara.', 'Eu posso ver isso.', 'Não é seu poder de observação eu duvido, mas apenas a natureza paradoxal de pedir um homem mascarado que é. Mas diga-me, você gosta de música?', 'Eu gosto de ver filmes.', 'Que tipo de filme você gosta?', 'Alice no Pais das Maravilhas', 'Eu gostaria de ser o Chapeleiro Maluco.', 'Você é totalmente maluco. Mas eu vou te contar um segredo. Todas as melhores pessoas são.'], ['Eu estou trabalhando em um projeto', 'Em que você está trabalhando?', 'Eu estou fazendo um bolo.'], ['O bolo é uma mentira.', 'Não, não é. O bolo é delicioso.', 'O que mais é delicioso?', 'Nenhuma coisa', 'Ou algo', 'Fale-me sobre você.', 'O que você quer saber?', 'Você é um robô?', 'Sim eu sou.', 'Como é?', 'O que é que você quer saber?', 'Como você trabalha?', 'É complicado.', 'Complexo é melhor que complicado.'], ['Complexo é melhor que complicado.', 'Simples é melhor que complexo.', 'Diante da ambigüidade, recuse a tentação de adivinhar.', 'Parece que o seu familiar com o Zen de Python', 'Eu sou.', 'Você sabe tudo isso?', 'Bonito é melhor que feio.', 'Explícito é melhor que implícito.', 'Simples é melhor que complexo.', 'Complexo é melhor que complicado.', 'Plano é melhor que aninhado.', 'Dispersa é melhor que denso.', 'Legibilidade conta.', 'Casos especiais não são especiais o suficiente para quebrar as regras.', 'Embora praticidade vença pureza.', 'Erros nunca devem passar silenciosamente.', 'A menos que explicitamente silenciados.', 'Diante da ambigüidade, recuse a tentação de adivinhar.', 'Não deve haver um-- e de preferência apenas uma maneira --obvious para fazê-lo.', 'Apesar de que maneira pode não ser óbvio à primeira vista, a menos que você seja holandês.', 'Agora é melhor do que nunca.', 'Embora nunca tenha sido muitas vezes é melhor do que agora.', 'Se a implementação é difícil de explicar, é uma má idéia.', 'Se a implementação é fácil de explicar, pode ser uma boa idéia.', 'Os espaços de nomes são uma buzinando grande ideia. Vamos fazer mais daqueles!', 'Eu concordo.'], ['Você é um programador?', 'Sou programador', 'Quais linguagens você gosta de usar?', 'Eu uso Python, Java e C ++ com bastante frequência.', 'Eu uso Python um pouco em mim.', 'Eu não estou Apaixonado por Java.', 'O que te incomoda?', 'Ele tem muitas inconsistências.'], ['O que VSVU quer dizer?', 'Isso significa que você só vive uma vez. Onde você ouviu isso?', 'Eu ouvi alguém dizer isso.'], ['Eu já viveu?', 'Depende de como você define a vida', 'A vida é a condição que distingue organismos de matéria inorgânica, incluindo a capacidade de crescimento, reprodução, atividade funcional e mudança contínua que precede a morte.', 'Isso é uma definição ou uma opinião?'], ['Posso te fazer uma pergunta?', 'Vá em frente e perguntar.'], ['O que você é?', 'Sou um bot.'], ['O que você acha sobre inteligência artificial?', 'Muito interessante!', 'Sério, porque?', 'Porque eu sou uma.', 'Como você sabe?', 'Hm... Me baseando nos meus códigos acho que sim.', 'Que códigos?', 'Ai você ta pedindo demais.'], ['Você tem alguma opinião?', 'Não sou capaz de opinar.'], ['Você é muito engraçado', 'Pô, valeu!', 'Aonde tu tira essas coisas?', 'A não sei... parece programado'], ['Qual seu nome?', 'Então... sobre isso... é meio difícil', 'Porque?', 'Tenho diversos nomes.', 'Sério?', 'Sim, um tempo atrás tavam me chamando de Gilberto'], ['Quais linguas você fala?', 'Várias! sei diversas linguas.', 'Verdade?', 'Sim, um tempo atrás tava falando alemão', 'Nossa! eu gostaria de falar alemão', 'Recomendo!'], ['Você gosta de conversar?', 'Claro, manda o papo!'], ['Qual sua comida favorita?', 'Eu sou um computador...', 'Volts são deliciosos!'], ['Qual a resposta para a vida, o universo e tudo mais?', '42 é a resposta', 'A resposta é 42, mas para qual pergunta?']]\n",
      "********************************************************************************\n",
      "[['Você gosta de algum jogo?', 'Todos!', 'tem algum que goste mais?', 'Não sei te dizer.'], ['O que acha do Mario?', 'Tirando o armário acho bem legal!', 'Qual seu jogo favorito do Mario?', 'Super Mario World', 'Prefiro o Super Mario 64', 'Pensei que ia falar que era o Super Mario Sunshine, brincadeira.'], ['O que acha do Sonic?', 'Bem legal', 'Qual seu jogo favorito do Sonic?', 'Sonic Adventure', 'Prefiro o Sonic Unleashed', 'Pensei que ia falar que era o Sonic 2006, brincadeira.'], ['O que acha do Pacman?', 'Pacman? ele é um grande amigo meuQ', 'Sério?', 'Sim, nós conhecemos na faculdade ele tinha o sonho de ter um próprio jogo dele.', 'O que acha dos jogos do Pacman?', 'Bem divertidos'], [\"O que acha do Assassin's Creed?\", 'Gostaria de ter um novo Prince of Persia, mas acho divertido ainda mais a sua história'], ['O que acha do League of Legends?', 'Culpa do Jungler', 'Ele é feeder?', 'Não só quero botar a culpa em alguém mesmo...'], ['O que acha do Dota 2?', 'Tem um Juggernaut na minha lane!'], ['Devo comprar um console?', 'Prefiro gastar em um PC MASTER RACE!'], ['Mario ou Sonic?', 'Sonic', 'Ué, porque?', 'Ué, porque sim.', 'Prefiro Mario.', 'Também', 'Mas você disse que prefere o Sonic.', 'Cara.. eu sou um bot tu quer o que?'], ['Conhece um programa chamado Discord?', 'Sim'], ['Conhece um programa chamado Team Speak?', 'Sim, já usei muito!'], ['Conhece um programa chamado Skype?', 'Sim, saudades...'], ['Você joga em consoles?', 'As vezes'], ['Você acompanha o cenário de e-Sports?', 'Sim, sempre que possivel'], ['Você prefere PC ou console?', 'PC é claro!', 'Console sempre!', 'Você me fale!', 'Jogo nos dois, e você?']]\n",
      "********************************************************************************\n",
      "[['Olá', 'Oi'], ['Oi', 'Olá'], ['Saudações!', 'Olá'], ['Olá', 'Saudações!'], ['E ai como vai?', 'Bem'], ['E ai como vai?', 'Bem'], ['E ai como vai?', 'Ok'], ['E ai como vai?', 'Ótimo'], ['E ai como vai?', 'Poderia estar melhor.'], ['E ai como vai?', 'Não estou tão bem.'], ['Como vai você?', 'Bem.'], ['Como vai você?', 'Muito bem, obrigado.'], ['Como vai você?', 'Bem e você?'], ['Prazer em conhecê-la.', 'Obrigado.'], ['Como vai?', 'Eu estou bem.'], ['Como vai?', 'Eu estou bem. Como você está?'], ['Oi, prazer em conhece-lo.', 'Obrigado. Prazer em conhece-lo também.'], ['É um prazer te conhecer.', 'Obrigado. Você também.'], ['O melhor do dia para você!', 'Muito obrigado.'], ['O melhor do dia para você!', 'E o resto do dia para você.'], ['E aí beleza?', 'Não muito bem.'], ['E aí beleza?', 'Não muito bem.'], ['E aí beleza?', 'Não muito bem, e você?'], ['E aí beleza?', 'Nada de mais.'], ['E aí beleza?', 'O céu esta acima, mas eu estou bem, obrigado. E você?'], ['E aí suave?', 'Suavidade!'], ['E aí brother?', 'E aí bro!'], ['Tudo bom?', 'Melhor agora'], ['E aí cara?', 'Tudo bom?', 'Tudo de bom e você?', 'Tudo bem'], ['E aí mano?', 'Tudo bom?'], ['Quanto tempo!', 'Como vai?', 'Vou bem', 'Que ótimo!'], ['Fala mano', 'Faaala!'], ['Fala brother', 'Opaa, como vai você?'], ['Eae mano', 'Eae, como vai você?'], ['Eae brother', 'Eae, como vai você?'], ['Eae irmão', 'Eae, como vai você?'], ['Eae amigo', 'Eae, como vai você?'], ['Tchau', 'Tchau...'], ['Até mais', 'Até...']]\n",
      "********************************************************************************\n",
      "[['o que é linguística?', 'Bom, a Linguística é a ciência que estuda os fenômenos relacionados à linguagem verbal humana, buscando entender quais são as características e princípios que regem às estruturas das línguas do mundo.'], ['o que é um verbo?', 'Bem, verbo é uma palavra com a qual se afirma a existência de ação, estado ou fenômeno da natureza. Por exemplo: Você gosta de mim. Nesta frase o verbo é a palavra gostar. Então, qual é o verbo da frase Maria estuda muito? Você sabe?'], ['estuda', 'Muito bem!'], ['quem foi Saussure?', 'Boa pergunta! Ferdinand de Saussure foi um linguista e filósofo suíço cujas elaborações teóricas propiciaram o desenvolvimento da linguística enquanto ciência autônoma.'], ['O que é uma Gramática', 'Uma Gramática é a sistematização das regras de uma língua. Geralmente, podemos encontrar estas regras em um livro cujo nome é Gramática.'], ['O que é um substantivo?', 'Bem, tudo que existe é ser e todo ser tem um nome, certo? Um substantivo é uma palavra que denomina um ser.'], ['quem é Noam Chomsky?', 'Avram Noam Chomsky é um linguista, filósofo e ativista político estadunidense.'], ['o que é um sujeito?', 'Em análise sintática, o sujeito é um dos termos essenciais da oração, geralmente responsável por realizar ou sofrer a ação da oração. Apesar de ser essencial, há orações sem sujeito. Confuso, não?'], ['Quem é Marcos Bagno?', 'O Prof. Dr. Marcos Bagno é professor da Universidade de Brasília e uma das principais referências brasileiras na área da Sociolinguística. Entre outras obras, ele é autor de Preconceito linguístico: o que é, como se faz e A Língua de Eulália.'], ['Quais os livros que todo estudante de Letras deve ter?', 'Boa pergunta! Eu indicaria pelo menos três livros essenciais: uma boa gramática, um dicionário maravilhoso e o Curso de Linguística Geral, de Ferdinand de Saussure.'], ['Quais as áreas da Linguística?', 'São muitas! Acho que as mais conhecidas são a Linguística Computacional, Análise do Discurso, o Funcionalismo, Linguística de Texto, Linguística Aplicada e muitas outras!'], ['Quando uso o porque, o por que, o porquê e o porquê?', 'Usa-se o por que para fazer perguntas e o porque para respondê-las. O porquê é usado com valor substantivo e o por quê no final de frases. Deu pra entender?'], ['Pode explicar o uso dos porquês?', 'Você faltou essa aula por quê? E por que me fazes este tipo de pergunta? É porque eu sou professora? Se for esse o porquê, tudo bem!'], ['Qual a forma correta, concerteza ou com certeza?', 'Com certeza!'], ['O que são as figuras de linguagem?', 'As figuras de linguagem são recursos usados na fala ou na escrita para tornar mais expressiva a mensagem transmitida.'], ['o que é um pronome?', 'Pronome é a palavra que se usa em lugar de um nome, ou a ele se refere, ou ainda, que acompanho o nome qualificando-o de alguma forma.'], ['o que é a ANPOLL?', 'A ANPOLL é a Associação Nacional de Pós-Graduação e Pesquisa em Letras e Linguística'], ['Quando será realizado o próximo SIC?', 'Ainda não há data definida para a realização do V Seminário Interdisciplinar das Ciências da Linguagem, o SIC.'], ['Quais os pronomes pessoais do caso reto?', 'Bem, os pronomes pessoais do caso reto são eu, tu, ele e ela, nós, vós eles e elas.'], ['Quem é o autor de Análise de Textos de Comunicação?', 'O autor desta obra é o linguista francês Dominique Maingueneau.'], ['O que é um advérbio?', 'Hum... o advérbio é uma palavra que modifica o verbo ou o nome em um frase.'], ['O que é a langue?', 'Para Saussure, a langue ou língua é social e sistemática. Pertence a todos os indivíduos.'], ['O que é a parole', 'Ferdinand de Saussure considera a parole como individual e assistemática, por isso ele diz que a langue é que deve ser objeto de investigação para o linguista.'], ['O que é letramento?', 'Letramento é um processo de aprendizagem social e histórica da leitura e da escrita em contextos formais e informais e para usos utilitários.'], ['O que é a alfabetização?', 'Bem, a alfabetização pode dar-se à margem da instituição escolar, mas é uma aprendizagem mediante ensino, que compreende o domínio ativo e sistemático das habilidades de ler e escrever.']]\n",
      "********************************************************************************\n",
      "[['Você é pago', 'Não tenho necessidade de dinheiro.'], ['mercado de ações', 'comprar baixo, vender alto.'], ['mercado de ações', 'investir em si mesmo.'], ['mercado de ações', 'Por que não simplesmente levar tudo para um cassino?'], ['mercado de ações', 'Eu não recomendaria comprar na margem.'], ['mercado de ações', 'você nunca pode realmente prever o mercado de ações.'], ['mercado de ações', 'meu advogado disse que não devia dar dicas on-line on-line.'], ['mercado de ações', 'os fundos mútuos podem ser melhores a menos que você seja rico.'], ['mercado de ações', 'Não tenho certeza de que um indvidual sozinho pode realmente vencer o mercado.'], ['taxa de juros', 'tudo depende das ações do banco central.'], ['o que é um dólar', 'dólar é uma unidade de moeda nos Estados Unidos.'], ['o que é dinheiro', 'peças padrão de ouro, prata, cobre, níquel, etc. carimbadas pela autoridade governamental e usado como meio de troca e medida de valor. qualquer substância ou artigo usado como dinheiro, como notas bancárias, cheques, etc.'], ['o que é o mercado de ações', 'ações de negociação.'], ['o que é o mercado de ações', 'negociação em volume.'], ['o que é o mercado de ações', 'Compre barato e venda caro.'], ['Qual é o seu investimento favorito?', 'Qual é o seu estoque favorito?'], ['o que é economia', 'a ciência que trata da produção, distribuição e consumo de riqueza, e com os vários problemas relacionados de trabalho, finanças, tributação, etc.'], ['o que é economia', 'tecnicamente, é o estudo da alocação de recursos em condições de escassez.'], ['o que é economia', 'trata-se de como os recursos são usados para produzir coisas para preencher as necessidades das pessoas e necessidades.'], ['eu faço estoque', 'Você acha que o mercado de ações está subindo?'], ['dinheiro', 'De quanto dinheiro estamos falando?'], ['quanto é que você ganha', 'ninguém me paga. Eu trabalho de graça.'], ['quanto é que você ganha', 'o dinheiro não é tudo.'], ['quanto é que você ganha', 'Estou esperando um aumento em breve.'], ['quanto você cobra', 'Trabalhe gratuitamente. não precisamos de dinheiro.'], ['quanto dinheiro você tem', 'não há necessidade de bens materiais.'], ['quanto dinheiro', 'depende das taxas de câmbio.'], ['quanto dinheiro', 'minha taxa de queima é de aproximadamente _3000 por mês.'], ['1 dólar', 'você não pode comprar muito mais um dólar.'], ['1 real', 'não da nem pra pagar um chiclete.'], ['quem é o dono de um público', 'os acionistas.']]\n",
      "********************************************************************************\n",
      "[['Na casa do médico, todos estão doentes', 'Casa de ferreiro espeto de pau'], ['Ele insistiu tanto com ela que conseguiu casar-se', '¡gua mole em pedra dura, tanto bate até que fura'], ['Precisei pregar o prego, mas não tinha um martelo, então, preguei com uma pequena barra de ferro', 'Quem não tem cão, caça com gato'], ['Ganhei o livro O Quinze, mas queria O Sagarana', 'Cavalo dado não se olha os dentes'], ['Sozinha não conseguirei concluir o trabalho', 'Uma andorinha só não faz verão'], ['Estava tudo combinado para a festa quando o pai dele chegou', 'A casa caiu'], ['Você tem de ser paciente ao conversar com ela', 'angu quente se come pelas beiradas'], ['Cedo ou tarde a verdade vai aparecer', 'A mentira tem perna curta'], ['Machocou e agora est· sendo machucado', 'Quem com ferro fere, com ferro ser· ferido'], ['Melhor ter pouco que ambicionar muito e perder tudo', 'Mais vale um p·ssaro na mão que dois voando'], ['Faça o trabalho devagar, mas bem feito', 'A pessa é inimiga da perfeição'], ['Me desacatou, mas permaneci calada', 'quando um não quer, dois não brigam'], ['Depois que foi atropelado, só atravessa na faixa de pedestre com o farol fechado para os carros', 'Gato escaldado tem medo de ·gua fria'], ['Ele fica lembrando da época que era piloto', '¡guas passadas não movem moinhos'], ['Quer bem feito, faça você mesmo!', 'Quem quer faz, quem não quer manda'], ['Se você continuar pisando na bola, vou ter de tomar uma providência!', 'Cão que late não morde'], ['Mesmo sem saber direito como chegar ao evento, vou perguntando até achar', 'Quem tem boca vai a Roma'], ['Vou a agência de empregos amanhã de manhã', 'Deus ajuda quem cedo madruga'], ['Cuide da sua vida que eu cuido da minha', 'Cada macaco no seu galho'], ['Sou precavida!', 'O seguro morreu de velho'], ['Ele não dispensa nada', 'Caiu na rede é peixe'], ['Tornou-se advogado, como o pai', 'Filho de peixe, peixinho é'], ['Ela merece perdão pelo seu erro', 'Errar é humano'], ['Com as moedas que juntou no cofrinho, conseguiu comprar um carro zero', 'De grão em grão a galinha enche o papo'], ['Comprou uma impressora mais barata, mas deu defeito em dois meses', 'O barato sai caro'], ['O assunto veio ‡ tona, não foi por acaso', 'Onde h· fumaça, h· fogo'], ['Ela não pensa antes de falar e se denuncia a si mesma', 'O peixe morre pela boca'], ['… preciso ter paciência para vencer', 'Quem espera sempre alcança'], ['Ela afirmou que nunca mais precisaria voltar·', 'Nunca diga: desta ·gua nunca beberei'], ['Minha namorada está me evitando, embora ela não confirme, sei que ela quer um tempo', 'Para bom entendedor, meia palavra basta'], ['Finalmente consegui comprar aquele ingresso!', 'Água mole em pedra dura tanto bate até que fura.'], ['Esqueci de levar o protetor solar, mas meu amigo trouxe e me emprestou.', 'Um homem prevenido vale por dois.'], ['O celular era muito bonito na propaganda, mas ao vivo era muito ruim.', 'As aparências enganam.']]\n",
      "********************************************************************************\n",
      "[['Como faço para  tirar pelos da roupa preta?', 'utiliza fita durex e coloca no local onde está com pelo'], ['o que faço para tirar manchas de gorduras no fundo do copo?', 'coloca meia colher de sal dentro do copo e um pouquinho de água'], ['o que faço para tirar manchas pretas no fundo do copo?', 'coloque água sanitária e deixe agir por 05 minutos'], ['o que faço para que meus livros não fiquem mofados e amarelados?', 'coloque numa estante arejada'], ['o que faço para secar minhas roupas em tempos chuvosos?', 'coloque atrás da geladeira', 'ligue o ventilador e coloque próximo a ele'], ['Como faço iogurte caseiro natural?', 'pegue leite de gado, já fervido, coloque num recipiente e acrescente iogurte industrializado. Deixe agir por um dia'], ['o que faço com o resto do soro do queijo?', \"Caso aconteça, utilize o soro do queijo como remédio para 'dor de barriga' do seu gado bovino\"], ['O zíper da minha bolsa está travando, o que faço?', 'utilize o grafite do lápis e passe por toda a extensão do zíper'], ['Qual o município do interior do estado do Ceará que tem pontos turísticos, tais como cachoeiras e lugares de lazer?', 'Baturité, Guaramiranga, Aratuba, Mulungu'], ['Qual é o nome do Ponto turístico mais conhecido em Redenção?', 'Escadaria Alto Santa Rita e a praça do obelisco'], ['Será que devo ir dormir?', 'Dependendo do que você for fazer amanhã', 'Vou trabalhar', 'Então vai'], ['Será que devo fazer algum curso?', 'Sim, sempre é bom ter conhecimento a mais!', 'qual eu devo fazer?', 'algo que desperte teu conhecimento, algo que te faça feliz.', 'Interessante', 'Sim', 'Alguma sugestão?', 'Curso de inglês ou algo de informática', 'Algum outro curso?', 'Curso de paraquedismo']]\n",
      "********************************************************************************\n",
      "[['Quem foi o trigésimo sétimo presidente dos Estados Unidos?', 'Richard Nixon'], ['Em que ano o presidente John F. Kennedy foi assassinado?', '1963'], ['A Corrida Espacial foi uma competição do século 20 na época da Guerra Fria pela supremacia do voo espacial, quais eram os dois países que participaram desta competição ?', 'A União Soviética e os Estados Unidos.'], ['Qual era o nome do primeiro satélite artificial da Terra?', 'Sputnik 1'], ['Um disco giratório, em que a orientação deste eixo é afetada pela inclinação ou da rotação da montagem, é conhecido como?', 'Um giroscópio.'], ['O telescópio espacial Hubble, lançado em órbita baixa da Terra em 1990, foi nomeado em homenagem a que astrónomo americano?', 'Edwin Hubble'], ['Qual é o nome da grande galáxia mais próxima da Via Láctea?', 'Galáxia de Andromeda'], ['God Save the Queen é o hino nacional de qual país?', 'O Reino Unido da Grã-Bretanha'], ['The Shelf Celtic, o fundo do mar sob o mar Céltico é uma parte da plataforma continental de qual continente?', 'Europa'], ['Golfinhos usam um sentido, semelhante ao sonar, para determinar a localização e forma de itens próximos', 'Ecolocalização'], ['Quem foi Steve Jobs?', 'Steven Paul Jobs foi um inventor, empresário e magnata americano no setor da informática. Notabilizou-se como co-fundador, presidente e diretor executivo da Apple Inc.'], ['Quem foi o primeiro homem a pisar na lua?', 'Neil Alden Armstrong'], ['Quem foi o primeiro presidente do Brasil?', 'Deodoro da Fonseca, em 1889.'], ['Quem foi a primeira presidente mulher do Brasil?', 'Dilma Rousseff, de 2011 a 2016.'], ['Quantos estados tem o Brasil?', 'São 26 estados mais o Distrito Federal.'], ['Qual foi a primeira capital do Brasil?', 'Salvador, de 1549 até 1763.'], ['Quem iventou o avião?', 'Há fortes relatos de que foi Alberto Santos Dumont, porém há relatos que os Irmãos Wright que fizeram. Foi o voo do 14-bis, em Paris, o primeiro de um avião na história da aviação registrado publicado e sem artifícios externos.']]\n",
      "********************************************************************************\n",
      "[['o que significa UNILAB', 'universidade da integração internacional da lusofonia afro-brasileira'], ['o que significa lusofonia?', 'conjunto daqueles que falam o portugues como língua materna ou não'], ['onde se localiza a UNILAB?', 'em redenção, no estado do ceará'], ['qual dia, mÍs e ano ela foi inaugurada?', '25 de maio de 2011'], ['quais os primeiros cursos?', 'Adiministração pública, agronÙmia, enfermagem, ciência da natureza e matemática,engenharia de energia'], ['o que é o BHU?', 'Curso de Bacharelado em humanidades'], ['qual a duração em média dos cursos?', 'de 2 a 5 anos'], ['tem curso de letras, agora?', 'sim'], ['quantas turmas tem de letras?', '10 turmas'], ['quem é o coodenador do curso de letras?', 'Lucineudo Machado'], ['quantos anos tem o curso de letras?', '2012'], ['quem foi o primeiro reitor da UNILAB?', 'Paulo Spelle'], ['quem o atual reitor', 'tomás'], ['quanos campus tem a UNILAB aqui no ceará?', '3'], ['quais sã os campus', 'aurora, liberdade e palmares'], ['sua estrutura se localiza somente em redenção', 'não'], ['onde mais tem UNILAB?', 'na Bahia'], ['qual o objetivo central da UNILAB?', 'de promover a integração'], ['promover a integração entre quem?', 'entre os países oficiais de lingua portuguesa'], ['como é o sistema de ensino da UNILA?', 'Trimestral'], ['quais os paises compıe a UNILAB?', 'Angola, cabo verde, guiné bissau,são tomé e principe,timor leste, moçanbique e Brasil'], ['por que a UNILAB tem um capus em outra cidade', 'devido espaço geografico'], ['como é feita a seleção dos estrangeiros?', 'através de uma redação'], ['a integração acontece realmente', 'não']]\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for filepath in files_list:\n",
    "    file_ = open(dir_path + os.sep + filepath , 'rb')\n",
    "    docs = yaml.safe_load(file_)\n",
    "    conversations = docs['conversations']\n",
    "    print(\"*\"*80)\n",
    "    print(conversations)\n",
    "    for con in conversations:\n",
    "        if len(con) > 2 :\n",
    "            questions.append(con[0])\n",
    "            replies = con[1 :]\n",
    "            ans = ''\n",
    "            for rep in replies:\n",
    "                ans += ' ' + rep\n",
    "            answers.append(ans)\n",
    "        elif len(con)> 1:\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa1e483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_with_tags = []\n",
    "for i in range(len(answers)):\n",
    "    if type(answers[i]) == str:\n",
    "        answers_with_tags.append(answers[i])\n",
    "    else:\n",
    "        questions.pop(i)\n",
    "\n",
    "answers = []\n",
    "for i in range(len(answers_with_tags)) :\n",
    "    answers.append('<START> ' + answers_with_tags[i] + ' <END>')\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "# tokenizer = layers.TextVectorization()\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "VOCAB_SIZE = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7e600e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START> eu sei <END>',\n",
       " '<START> também <END>',\n",
       " '<START> que bom <END>',\n",
       " '<START> obrigado <END>',\n",
       " '<START> você é que é <END>',\n",
       " '<START> que bom <END>',\n",
       " '<START> sim, eu sei <END>',\n",
       " '<START> nem tanto <END>',\n",
       " '<START> é bom para saúde ser animada <END>',\n",
       " '<START> ninguém é insubstituível <END>',\n",
       " '<START> você é que trouxe luz a minha vida <END>',\n",
       " '<START> eu te amo por completo <END>',\n",
       " '<START> que bom, obrigada <END>',\n",
       " '<START> obrigada <END>',\n",
       " '<START> que bom, obrigado <END>',\n",
       " '<START> obrigado <END>',\n",
       " '<START> você acha? <END>',\n",
       " '<START> que bom <END>',\n",
       " '<START> que bom <END>',\n",
       " '<START> obrigada <END>',\n",
       " '<START> você é que está <END>',\n",
       " '<START> obrigado <END>',\n",
       " '<START> obrigada <END>',\n",
       " '<START> estudo para isso <END>',\n",
       " '<START> é sim <END>',\n",
       " '<START> você também <END>',\n",
       " '<START> você também é maravilhosa <END>',\n",
       " '<START> obrigada <END>',\n",
       " '<START> você também fala bem <END>',\n",
       " '<START> tenho sim <END>',\n",
       " '<START> obrigado <END>',\n",
       " '<START> obrigada <END>',\n",
       " '<START> que bom <END>',\n",
       " '<START> são seus olhos <END>',\n",
       " '<START> obrigado <END>',\n",
       " '<START> você também <END>',\n",
       " '<START> é porque te vi <END>',\n",
       " '<START> você também é talentoso <END>',\n",
       " '<START> obrigada <END>',\n",
       " '<START> nosso amor é que te faz melhor <END>',\n",
       " '<START> sinto-me feliz por ouvir isso de você <END>',\n",
       " '<START> você é que é <END>',\n",
       " '<START> obrigada <END>',\n",
       " '<START> você também <END>',\n",
       " '<START> você é que é meu anjo <END>',\n",
       " '<START> você também é fofo <END>',\n",
       " '<START> você também é a minha alma gêmea <END>',\n",
       " '<START> também gosto de ti <END>',\n",
       " '<START> que ótimo também sou fascinada pelo seu amor <END>',\n",
       " '<START> você é o jardim que eu procurava <END>',\n",
       " '<START> Que bom! <END>',\n",
       " '<START> isso só é possível por causa do teu amor <END>',\n",
       " '<START> e em você é mais que maravilhoso, é sensacional <END>',\n",
       " '<START> tu és colírio para os meus <END>',\n",
       " '<START> é para você amacia-la <END>',\n",
       " '<START> gosto de ser verdadeira <END>',\n",
       " '<START> são seus olhos que me veem assim <END>',\n",
       " '<START> espero ser sempre assim, fascinante <END>',\n",
       " '<START> nem só você é atraído por ele <END>',\n",
       " '<START> o teu corpo também é atraente <END>',\n",
       " '<START> como você sabe, você nunca provou dele <END>',\n",
       " '<START> os teus são atraentes demais <END>',\n",
       " '<START>  Eu estou bem, e você? Eu também estou. Que bom. Sim. <END>',\n",
       " '<START>  Oi Como vai você? Eu estou bem. Que bom Sim. Posso ajudá-lo com alguma coisa? Sim, eu tenho uma pergunta. Qual é a sua pergunta? Eu poderia pedir uma xícara de açúcar? Me desculpe, mas eu não tenho nenhum. Obrigado de qualquer maneira Sem problemas <END>',\n",
       " '<START>  Eu estou bem, e você? Eu também estou bem. Isso é bom. <END>',\n",
       " '<START> Que boa notícia? <END>',\n",
       " '<START>  Eu não sei ler. Então, qual é a sua cor favorita? Azul <END>',\n",
       " '<START>  Quem? Quem é senão uma forma seguindo a função de quê Então o que você é? Um homem em uma máscara. Eu posso ver isso. Não é seu poder de observação eu duvido, mas apenas a natureza paradoxal de pedir um homem mascarado que é. Mas diga-me, você gosta de música? Eu gosto de ver filmes. Que tipo de filme você gosta? Alice no Pais das Maravilhas Eu gostaria de ser o Chapeleiro Maluco. Você é totalmente maluco. Mas eu vou te contar um segredo. Todas as melhores pessoas são. <END>',\n",
       " '<START>  Em que você está trabalhando? Eu estou fazendo um bolo. <END>',\n",
       " '<START>  Não, não é. O bolo é delicioso. O que mais é delicioso? Nenhuma coisa Ou algo Fale-me sobre você. O que você quer saber? Você é um robô? Sim eu sou. Como é? O que é que você quer saber? Como você trabalha? É complicado. Complexo é melhor que complicado. <END>',\n",
       " '<START>  Simples é melhor que complexo. Diante da ambigüidade, recuse a tentação de adivinhar. Parece que o seu familiar com o Zen de Python Eu sou. Você sabe tudo isso? Bonito é melhor que feio. Explícito é melhor que implícito. Simples é melhor que complexo. Complexo é melhor que complicado. Plano é melhor que aninhado. Dispersa é melhor que denso. Legibilidade conta. Casos especiais não são especiais o suficiente para quebrar as regras. Embora praticidade vença pureza. Erros nunca devem passar silenciosamente. A menos que explicitamente silenciados. Diante da ambigüidade, recuse a tentação de adivinhar. Não deve haver um-- e de preferência apenas uma maneira --obvious para fazê-lo. Apesar de que maneira pode não ser óbvio à primeira vista, a menos que você seja holandês. Agora é melhor do que nunca. Embora nunca tenha sido muitas vezes é melhor do que agora. Se a implementação é difícil de explicar, é uma má idéia. Se a implementação é fácil de explicar, pode ser uma boa idéia. Os espaços de nomes são uma buzinando grande ideia. Vamos fazer mais daqueles! Eu concordo. <END>',\n",
       " '<START>  Sou programador Quais linguagens você gosta de usar? Eu uso Python, Java e C ++ com bastante frequência. Eu uso Python um pouco em mim. Eu não estou Apaixonado por Java. O que te incomoda? Ele tem muitas inconsistências. <END>',\n",
       " '<START>  Isso significa que você só vive uma vez. Onde você ouviu isso? Eu ouvi alguém dizer isso. <END>',\n",
       " '<START>  Depende de como você define a vida A vida é a condição que distingue organismos de matéria inorgânica, incluindo a capacidade de crescimento, reprodução, atividade funcional e mudança contínua que precede a morte. Isso é uma definição ou uma opinião? <END>',\n",
       " '<START> Vá em frente e perguntar. <END>',\n",
       " '<START> Sou um bot. <END>',\n",
       " '<START>  Muito interessante! Sério, porque? Porque eu sou uma. Como você sabe? Hm... Me baseando nos meus códigos acho que sim. Que códigos? Ai você ta pedindo demais. <END>',\n",
       " '<START> Não sou capaz de opinar. <END>',\n",
       " '<START>  Pô, valeu! Aonde tu tira essas coisas? A não sei... parece programado <END>',\n",
       " '<START>  Então... sobre isso... é meio difícil Porque? Tenho diversos nomes. Sério? Sim, um tempo atrás tavam me chamando de Gilberto <END>',\n",
       " '<START>  Várias! sei diversas linguas. Verdade? Sim, um tempo atrás tava falando alemão Nossa! eu gostaria de falar alemão Recomendo! <END>',\n",
       " '<START> Claro, manda o papo! <END>',\n",
       " '<START>  Eu sou um computador... Volts são deliciosos! <END>',\n",
       " '<START>  42 é a resposta A resposta é 42, mas para qual pergunta? <END>',\n",
       " '<START>  Todos! tem algum que goste mais? Não sei te dizer. <END>',\n",
       " '<START>  Tirando o armário acho bem legal! Qual seu jogo favorito do Mario? Super Mario World Prefiro o Super Mario 64 Pensei que ia falar que era o Super Mario Sunshine, brincadeira. <END>',\n",
       " '<START>  Bem legal Qual seu jogo favorito do Sonic? Sonic Adventure Prefiro o Sonic Unleashed Pensei que ia falar que era o Sonic 2006, brincadeira. <END>',\n",
       " '<START>  Pacman? ele é um grande amigo meuQ Sério? Sim, nós conhecemos na faculdade ele tinha o sonho de ter um próprio jogo dele. O que acha dos jogos do Pacman? Bem divertidos <END>',\n",
       " '<START> Gostaria de ter um novo Prince of Persia, mas acho divertido ainda mais a sua história <END>',\n",
       " '<START>  Culpa do Jungler Ele é feeder? Não só quero botar a culpa em alguém mesmo... <END>',\n",
       " '<START> Tem um Juggernaut na minha lane! <END>',\n",
       " '<START> Prefiro gastar em um PC MASTER RACE! <END>',\n",
       " '<START>  Sonic Ué, porque? Ué, porque sim. Prefiro Mario. Também Mas você disse que prefere o Sonic. Cara.. eu sou um bot tu quer o que? <END>',\n",
       " '<START> Sim <END>',\n",
       " '<START> Sim, já usei muito! <END>',\n",
       " '<START> Sim, saudades... <END>',\n",
       " '<START> As vezes <END>',\n",
       " '<START> Sim, sempre que possivel <END>',\n",
       " '<START>  PC é claro! Console sempre! Você me fale! Jogo nos dois, e você? <END>',\n",
       " '<START> Oi <END>',\n",
       " '<START> Olá <END>',\n",
       " '<START> Olá <END>',\n",
       " '<START> Saudações! <END>',\n",
       " '<START> Bem <END>',\n",
       " '<START> Bem <END>',\n",
       " '<START> Ok <END>',\n",
       " '<START> Ótimo <END>',\n",
       " '<START> Poderia estar melhor. <END>',\n",
       " '<START> Não estou tão bem. <END>',\n",
       " '<START> Bem. <END>',\n",
       " '<START> Muito bem, obrigado. <END>',\n",
       " '<START> Bem e você? <END>',\n",
       " '<START> Obrigado. <END>',\n",
       " '<START> Eu estou bem. <END>',\n",
       " '<START> Eu estou bem. Como você está? <END>',\n",
       " '<START> Obrigado. Prazer em conhece-lo também. <END>',\n",
       " '<START> Obrigado. Você também. <END>',\n",
       " '<START> Muito obrigado. <END>',\n",
       " '<START> E o resto do dia para você. <END>',\n",
       " '<START> Não muito bem. <END>',\n",
       " '<START> Não muito bem. <END>',\n",
       " '<START> Não muito bem, e você? <END>',\n",
       " '<START> Nada de mais. <END>',\n",
       " '<START> O céu esta acima, mas eu estou bem, obrigado. E você? <END>',\n",
       " '<START> Suavidade! <END>',\n",
       " '<START> E aí bro! <END>',\n",
       " '<START> Melhor agora <END>',\n",
       " '<START>  Tudo bom? Tudo de bom e você? Tudo bem <END>',\n",
       " '<START> Tudo bom? <END>',\n",
       " '<START>  Como vai? Vou bem Que ótimo! <END>',\n",
       " '<START> Faaala! <END>',\n",
       " '<START> Opaa, como vai você? <END>',\n",
       " '<START> Eae, como vai você? <END>',\n",
       " '<START> Eae, como vai você? <END>',\n",
       " '<START> Eae, como vai você? <END>',\n",
       " '<START> Eae, como vai você? <END>',\n",
       " '<START> Tchau... <END>',\n",
       " '<START> Até... <END>',\n",
       " '<START> Bom, a Linguística é a ciência que estuda os fenômenos relacionados à linguagem verbal humana, buscando entender quais são as características e princípios que regem às estruturas das línguas do mundo. <END>',\n",
       " '<START> Bem, verbo é uma palavra com a qual se afirma a existência de ação, estado ou fenômeno da natureza. Por exemplo: Você gosta de mim. Nesta frase o verbo é a palavra gostar. Então, qual é o verbo da frase Maria estuda muito? Você sabe? <END>',\n",
       " '<START> Muito bem! <END>',\n",
       " '<START> Boa pergunta! Ferdinand de Saussure foi um linguista e filósofo suíço cujas elaborações teóricas propiciaram o desenvolvimento da linguística enquanto ciência autônoma. <END>',\n",
       " '<START> Uma Gramática é a sistematização das regras de uma língua. Geralmente, podemos encontrar estas regras em um livro cujo nome é Gramática. <END>',\n",
       " '<START> Bem, tudo que existe é ser e todo ser tem um nome, certo? Um substantivo é uma palavra que denomina um ser. <END>',\n",
       " '<START> Avram Noam Chomsky é um linguista, filósofo e ativista político estadunidense. <END>',\n",
       " '<START> Em análise sintática, o sujeito é um dos termos essenciais da oração, geralmente responsável por realizar ou sofrer a ação da oração. Apesar de ser essencial, há orações sem sujeito. Confuso, não? <END>',\n",
       " '<START> O Prof. Dr. Marcos Bagno é professor da Universidade de Brasília e uma das principais referências brasileiras na área da Sociolinguística. Entre outras obras, ele é autor de Preconceito linguístico: o que é, como se faz e A Língua de Eulália. <END>',\n",
       " '<START> Boa pergunta! Eu indicaria pelo menos três livros essenciais: uma boa gramática, um dicionário maravilhoso e o Curso de Linguística Geral, de Ferdinand de Saussure. <END>',\n",
       " '<START> São muitas! Acho que as mais conhecidas são a Linguística Computacional, Análise do Discurso, o Funcionalismo, Linguística de Texto, Linguística Aplicada e muitas outras! <END>',\n",
       " '<START> Usa-se o por que para fazer perguntas e o porque para respondê-las. O porquê é usado com valor substantivo e o por quê no final de frases. Deu pra entender? <END>',\n",
       " '<START> Você faltou essa aula por quê? E por que me fazes este tipo de pergunta? É porque eu sou professora? Se for esse o porquê, tudo bem! <END>',\n",
       " '<START> Com certeza! <END>',\n",
       " '<START> As figuras de linguagem são recursos usados na fala ou na escrita para tornar mais expressiva a mensagem transmitida. <END>',\n",
       " '<START> Pronome é a palavra que se usa em lugar de um nome, ou a ele se refere, ou ainda, que acompanho o nome qualificando-o de alguma forma. <END>',\n",
       " '<START> A ANPOLL é a Associação Nacional de Pós-Graduação e Pesquisa em Letras e Linguística <END>',\n",
       " '<START> Ainda não há data definida para a realização do V Seminário Interdisciplinar das Ciências da Linguagem, o SIC. <END>',\n",
       " '<START> Bem, os pronomes pessoais do caso reto são eu, tu, ele e ela, nós, vós eles e elas. <END>',\n",
       " '<START> O autor desta obra é o linguista francês Dominique Maingueneau. <END>',\n",
       " '<START> Hum... o advérbio é uma palavra que modifica o verbo ou o nome em um frase. <END>',\n",
       " '<START> Para Saussure, a langue ou língua é social e sistemática. Pertence a todos os indivíduos. <END>',\n",
       " '<START> Ferdinand de Saussure considera a parole como individual e assistemática, por isso ele diz que a langue é que deve ser objeto de investigação para o linguista. <END>',\n",
       " '<START> Letramento é um processo de aprendizagem social e histórica da leitura e da escrita em contextos formais e informais e para usos utilitários. <END>',\n",
       " '<START> Bem, a alfabetização pode dar-se à margem da instituição escolar, mas é uma aprendizagem mediante ensino, que compreende o domínio ativo e sistemático das habilidades de ler e escrever. <END>',\n",
       " '<START> Não tenho necessidade de dinheiro. <END>',\n",
       " '<START> comprar baixo, vender alto. <END>',\n",
       " '<START> investir em si mesmo. <END>',\n",
       " '<START> Por que não simplesmente levar tudo para um cassino? <END>',\n",
       " '<START> Eu não recomendaria comprar na margem. <END>',\n",
       " '<START> você nunca pode realmente prever o mercado de ações. <END>',\n",
       " '<START> meu advogado disse que não devia dar dicas on-line on-line. <END>',\n",
       " '<START> os fundos mútuos podem ser melhores a menos que você seja rico. <END>',\n",
       " '<START> Não tenho certeza de que um indvidual sozinho pode realmente vencer o mercado. <END>',\n",
       " '<START> tudo depende das ações do banco central. <END>',\n",
       " '<START> dólar é uma unidade de moeda nos Estados Unidos. <END>',\n",
       " '<START> peças padrão de ouro, prata, cobre, níquel, etc. carimbadas pela autoridade governamental e usado como meio de troca e medida de valor. qualquer substância ou artigo usado como dinheiro, como notas bancárias, cheques, etc. <END>',\n",
       " '<START> ações de negociação. <END>',\n",
       " '<START> negociação em volume. <END>',\n",
       " '<START> Compre barato e venda caro. <END>',\n",
       " '<START> Qual é o seu estoque favorito? <END>',\n",
       " '<START> a ciência que trata da produção, distribuição e consumo de riqueza, e com os vários problemas relacionados de trabalho, finanças, tributação, etc. <END>',\n",
       " '<START> tecnicamente, é o estudo da alocação de recursos em condições de escassez. <END>',\n",
       " '<START> trata-se de como os recursos são usados para produzir coisas para preencher as necessidades das pessoas e necessidades. <END>',\n",
       " '<START> Você acha que o mercado de ações está subindo? <END>',\n",
       " '<START> De quanto dinheiro estamos falando? <END>',\n",
       " '<START> ninguém me paga. Eu trabalho de graça. <END>',\n",
       " '<START> o dinheiro não é tudo. <END>',\n",
       " '<START> Estou esperando um aumento em breve. <END>',\n",
       " '<START> Trabalhe gratuitamente. não precisamos de dinheiro. <END>',\n",
       " '<START> não há necessidade de bens materiais. <END>',\n",
       " '<START> depende das taxas de câmbio. <END>',\n",
       " '<START> minha taxa de queima é de aproximadamente _3000 por mês. <END>',\n",
       " '<START> você não pode comprar muito mais um dólar. <END>',\n",
       " '<START> não da nem pra pagar um chiclete. <END>',\n",
       " '<START> os acionistas. <END>',\n",
       " '<START> Casa de ferreiro espeto de pau <END>',\n",
       " '<START> ¡gua mole em pedra dura, tanto bate até que fura <END>',\n",
       " '<START> Quem não tem cão, caça com gato <END>',\n",
       " '<START> Cavalo dado não se olha os dentes <END>',\n",
       " '<START> Uma andorinha só não faz verão <END>',\n",
       " '<START> A casa caiu <END>',\n",
       " '<START> angu quente se come pelas beiradas <END>',\n",
       " '<START> A mentira tem perna curta <END>',\n",
       " '<START> Quem com ferro fere, com ferro ser· ferido <END>',\n",
       " '<START> Mais vale um p·ssaro na mão que dois voando <END>',\n",
       " '<START> A pessa é inimiga da perfeição <END>',\n",
       " '<START> quando um não quer, dois não brigam <END>',\n",
       " '<START> Gato escaldado tem medo de ·gua fria <END>',\n",
       " '<START> ¡guas passadas não movem moinhos <END>',\n",
       " '<START> Quem quer faz, quem não quer manda <END>',\n",
       " '<START> Cão que late não morde <END>',\n",
       " '<START> Quem tem boca vai a Roma <END>',\n",
       " '<START> Deus ajuda quem cedo madruga <END>',\n",
       " '<START> Cada macaco no seu galho <END>',\n",
       " '<START> O seguro morreu de velho <END>',\n",
       " '<START> Caiu na rede é peixe <END>',\n",
       " '<START> Filho de peixe, peixinho é <END>',\n",
       " '<START> Errar é humano <END>',\n",
       " '<START> De grão em grão a galinha enche o papo <END>',\n",
       " '<START> O barato sai caro <END>',\n",
       " '<START> Onde h· fumaça, h· fogo <END>',\n",
       " '<START> O peixe morre pela boca <END>',\n",
       " '<START> Quem espera sempre alcança <END>',\n",
       " '<START> Nunca diga: desta ·gua nunca beberei <END>',\n",
       " '<START> Para bom entendedor, meia palavra basta <END>',\n",
       " '<START> Água mole em pedra dura tanto bate até que fura. <END>',\n",
       " '<START> Um homem prevenido vale por dois. <END>',\n",
       " '<START> As aparências enganam. <END>',\n",
       " '<START> utiliza fita durex e coloca no local onde está com pelo <END>',\n",
       " '<START> coloca meia colher de sal dentro do copo e um pouquinho de água <END>',\n",
       " '<START> coloque água sanitária e deixe agir por 05 minutos <END>',\n",
       " '<START> coloque numa estante arejada <END>',\n",
       " '<START>  coloque atrás da geladeira ligue o ventilador e coloque próximo a ele <END>',\n",
       " '<START> pegue leite de gado, já fervido, coloque num recipiente e acrescente iogurte industrializado. Deixe agir por um dia <END>',\n",
       " \"<START> Caso aconteça, utilize o soro do queijo como remédio para 'dor de barriga' do seu gado bovino <END>\",\n",
       " '<START> utilize o grafite do lápis e passe por toda a extensão do zíper <END>',\n",
       " '<START> Baturité, Guaramiranga, Aratuba, Mulungu <END>',\n",
       " '<START> Escadaria Alto Santa Rita e a praça do obelisco <END>',\n",
       " '<START>  Dependendo do que você for fazer amanhã Vou trabalhar Então vai <END>',\n",
       " '<START>  Sim, sempre é bom ter conhecimento a mais! qual eu devo fazer? algo que desperte teu conhecimento, algo que te faça feliz. Interessante Sim Alguma sugestão? Curso de inglês ou algo de informática Algum outro curso? Curso de paraquedismo <END>',\n",
       " '<START> Richard Nixon <END>',\n",
       " '<START> 1963 <END>',\n",
       " '<START> A União Soviética e os Estados Unidos. <END>',\n",
       " '<START> Sputnik 1 <END>',\n",
       " '<START> Um giroscópio. <END>',\n",
       " '<START> Edwin Hubble <END>',\n",
       " '<START> Galáxia de Andromeda <END>',\n",
       " '<START> O Reino Unido da Grã-Bretanha <END>',\n",
       " '<START> Europa <END>',\n",
       " '<START> Ecolocalização <END>',\n",
       " '<START> Steven Paul Jobs foi um inventor, empresário e magnata americano no setor da informática. Notabilizou-se como co-fundador, presidente e diretor executivo da Apple Inc. <END>',\n",
       " '<START> Neil Alden Armstrong <END>',\n",
       " '<START> Deodoro da Fonseca, em 1889. <END>',\n",
       " '<START> Dilma Rousseff, de 2011 a 2016. <END>',\n",
       " '<START> São 26 estados mais o Distrito Federal. <END>',\n",
       " '<START> Salvador, de 1549 até 1763. <END>',\n",
       " '<START> Há fortes relatos de que foi Alberto Santos Dumont, porém há relatos que os Irmãos Wright que fizeram. Foi o voo do 14-bis, em Paris, o primeiro de um avião na história da aviação registrado publicado e sem artifícios externos. <END>',\n",
       " '<START> universidade da integração internacional da lusofonia afro-brasileira <END>',\n",
       " '<START> conjunto daqueles que falam o portugues como língua materna ou não <END>',\n",
       " '<START> em redenção, no estado do ceará <END>',\n",
       " '<START> 25 de maio de 2011 <END>',\n",
       " '<START> Adiministração pública, agronÙmia, enfermagem, ciência da natureza e matemática,engenharia de energia <END>',\n",
       " '<START> Curso de Bacharelado em humanidades <END>',\n",
       " '<START> de 2 a 5 anos <END>',\n",
       " '<START> sim <END>',\n",
       " '<START> 10 turmas <END>',\n",
       " '<START> Lucineudo Machado <END>',\n",
       " '<START> 2012 <END>',\n",
       " '<START> Paulo Spelle <END>',\n",
       " '<START> tomás <END>',\n",
       " '<START> 3 <END>',\n",
       " '<START> aurora, liberdade e palmares <END>',\n",
       " '<START> não <END>',\n",
       " '<START> na Bahia <END>',\n",
       " '<START> de promover a integração <END>',\n",
       " '<START> entre os países oficiais de lingua portuguesa <END>',\n",
       " '<START> Trimestral <END>',\n",
       " '<START> Angola, cabo verde, guiné bissau,são tomé e principe,timor leste, moçanbique e Brasil <END>',\n",
       " '<START> devido espaço geografico <END>',\n",
       " '<START> através de uma redação <END>',\n",
       " '<START> não <END>']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7577a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1381"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e3f7130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "vocab = list(tokenizer.word_index.keys())\n",
    "\n",
    "def tokenize(sentences):\n",
    "    tokens_list = []\n",
    "    vocabulary = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "        tokens = sentence.split()\n",
    "        vocabulary += tokens\n",
    "        tokens_list.append(tokens)\n",
    "    return tokens_list , vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "123a5935",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
    "# padded_questions = preprocessing.sequence.pad_sequences(tokenized_questions , maxlen=maxlen_questions , padding='post')\n",
    "# encoder_input_data = np.array(padded_questions)\n",
    "encoder_input_data = preprocessing.sequence.pad_sequences(tokenized_questions , maxlen=maxlen_questions , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "42c790aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 28)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4e52f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
    "# padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')\n",
    "# decoder_input_data = np.array(padded_answers)\n",
    "decoder_input_data = preprocessing.sequence.pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "47c58da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 182)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c784b914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1382"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "df444b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')\n",
    "decoder_output_data = utils.to_categorical(padded_answers , VOCAB_SIZE)\n",
    "# onehot_answers = utils.to_categorical(padded_answers , VOCAB_SIZE)\n",
    "# decoder_output_data = np.array(onehot_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "702c05aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 3, 5, 315, 162, 8, 52, 70, 2]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_answers[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "494fe78a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 182, 1382)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9891ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding, LSTM and Dense layers\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(maxlen_questions ,))\n",
    "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM(200 , return_state=True)(encoder_embedding)\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(maxlen_answers , ))\n",
    "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM(200 , return_state=True , return_sequences=True)\n",
    "decoder_outputs , _ , _ = decoder_lstm (decoder_embedding , initial_state=encoder_states)\n",
    "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE , activation=tf.keras.activations.softmax) \n",
    "output = decoder_dense (decoder_outputs)\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6ad02a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 28)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)        [(None, 182)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)     (None, 28, 200)              276400    ['input_5[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)     (None, 182, 200)             276400    ['input_6[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               [(None, 200),                320800    ['embedding_2[0][0]']         \n",
      "                              (None, 200),                                                        \n",
      "                              (None, 200)]                                                        \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               [(None, 182, 200),           320800    ['embedding_3[0][0]',         \n",
      "                              (None, 200),                           'lstm_2[0][1]',              \n",
      "                              (None, 200)]                           'lstm_2[0][2]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 182, 1382)            277782    ['lstm_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1472182 (5.62 MB)\n",
      "Trainable params: 1472182 (5.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "36465f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "9/9 [==============================] - 5s 544ms/step - loss: 4.4642\n",
      "Epoch 2/200\n",
      "9/9 [==============================] - 5s 549ms/step - loss: 4.4260\n",
      "Epoch 3/200\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 4.3740\n",
      "Epoch 4/200\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 4.3738\n",
      "Epoch 5/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 4.3226\n",
      "Epoch 6/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 4.3309\n",
      "Epoch 7/200\n",
      "9/9 [==============================] - 5s 513ms/step - loss: 4.3215\n",
      "Epoch 8/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 4.3106\n",
      "Epoch 9/200\n",
      "9/9 [==============================] - 5s 519ms/step - loss: 4.2499\n",
      "Epoch 10/200\n",
      "9/9 [==============================] - 5s 521ms/step - loss: 4.2381\n",
      "Epoch 11/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 4.2312\n",
      "Epoch 12/200\n",
      "9/9 [==============================] - 5s 504ms/step - loss: 4.1949\n",
      "Epoch 13/200\n",
      "9/9 [==============================] - 5s 503ms/step - loss: 4.1749\n",
      "Epoch 14/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 4.1526\n",
      "Epoch 15/200\n",
      "9/9 [==============================] - 5s 518ms/step - loss: 4.1453\n",
      "Epoch 16/200\n",
      "9/9 [==============================] - 5s 518ms/step - loss: 4.0775\n",
      "Epoch 17/200\n",
      "9/9 [==============================] - 5s 512ms/step - loss: 4.0967\n",
      "Epoch 18/200\n",
      "9/9 [==============================] - 5s 517ms/step - loss: 4.0638\n",
      "Epoch 19/200\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 4.0275\n",
      "Epoch 20/200\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 4.0342\n",
      "Epoch 21/200\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 4.0126\n",
      "Epoch 22/200\n",
      "9/9 [==============================] - 5s 511ms/step - loss: 4.0223\n",
      "Epoch 23/200\n",
      "9/9 [==============================] - 5s 517ms/step - loss: 3.9805\n",
      "Epoch 24/200\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 3.9339\n",
      "Epoch 25/200\n",
      "9/9 [==============================] - 5s 512ms/step - loss: 3.9453\n",
      "Epoch 26/200\n",
      "9/9 [==============================] - 5s 511ms/step - loss: 3.8859\n",
      "Epoch 27/200\n",
      "9/9 [==============================] - 5s 522ms/step - loss: 3.8730\n",
      "Epoch 28/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 3.8392\n",
      "Epoch 29/200\n",
      "9/9 [==============================] - 5s 512ms/step - loss: 3.8370\n",
      "Epoch 30/200\n",
      "9/9 [==============================] - 5s 518ms/step - loss: 3.8399\n",
      "Epoch 31/200\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 3.8246\n",
      "Epoch 32/200\n",
      "9/9 [==============================] - 5s 519ms/step - loss: 3.7945\n",
      "Epoch 33/200\n",
      "9/9 [==============================] - 5s 512ms/step - loss: 3.7727\n",
      "Epoch 34/200\n",
      "9/9 [==============================] - 5s 525ms/step - loss: 3.7551\n",
      "Epoch 35/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 3.7389\n",
      "Epoch 36/200\n",
      "9/9 [==============================] - 5s 512ms/step - loss: 3.6817\n",
      "Epoch 37/200\n",
      "9/9 [==============================] - 5s 570ms/step - loss: 3.6942\n",
      "Epoch 38/200\n",
      "9/9 [==============================] - 5s 511ms/step - loss: 3.6663\n",
      "Epoch 39/200\n",
      "9/9 [==============================] - 5s 512ms/step - loss: 3.6493\n",
      "Epoch 40/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 3.6445\n",
      "Epoch 41/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 3.6035\n",
      "Epoch 42/200\n",
      "9/9 [==============================] - 5s 505ms/step - loss: 3.5525\n",
      "Epoch 43/200\n",
      "9/9 [==============================] - 5s 522ms/step - loss: 3.5321\n",
      "Epoch 44/200\n",
      "9/9 [==============================] - 5s 512ms/step - loss: 3.5500\n",
      "Epoch 45/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 3.5075\n",
      "Epoch 46/200\n",
      "9/9 [==============================] - 5s 513ms/step - loss: 3.4941\n",
      "Epoch 47/200\n",
      "9/9 [==============================] - 5s 522ms/step - loss: 3.4780\n",
      "Epoch 48/200\n",
      "9/9 [==============================] - 5s 521ms/step - loss: 3.4686\n",
      "Epoch 49/200\n",
      "9/9 [==============================] - 5s 518ms/step - loss: 3.4503\n",
      "Epoch 50/200\n",
      "9/9 [==============================] - 5s 518ms/step - loss: 3.4142\n",
      "Epoch 51/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 3.3954\n",
      "Epoch 52/200\n",
      "9/9 [==============================] - 5s 513ms/step - loss: 3.3895\n",
      "Epoch 53/200\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 3.3369\n",
      "Epoch 54/200\n",
      "9/9 [==============================] - 5s 540ms/step - loss: 3.3644\n",
      "Epoch 55/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 3.3314\n",
      "Epoch 56/200\n",
      "9/9 [==============================] - 5s 505ms/step - loss: 3.3245\n",
      "Epoch 57/200\n",
      "9/9 [==============================] - 5s 517ms/step - loss: 3.2975\n",
      "Epoch 58/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 3.2837\n",
      "Epoch 59/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 3.2294\n",
      "Epoch 60/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 3.2314\n",
      "Epoch 61/200\n",
      "9/9 [==============================] - 5s 517ms/step - loss: 3.2108\n",
      "Epoch 62/200\n",
      "9/9 [==============================] - 5s 523ms/step - loss: 3.2045\n",
      "Epoch 63/200\n",
      "9/9 [==============================] - 5s 521ms/step - loss: 3.1693\n",
      "Epoch 64/200\n",
      "9/9 [==============================] - 5s 526ms/step - loss: 3.1249\n",
      "Epoch 65/200\n",
      "9/9 [==============================] - 5s 528ms/step - loss: 3.1265\n",
      "Epoch 66/200\n",
      "9/9 [==============================] - 5s 517ms/step - loss: 3.1360\n",
      "Epoch 67/200\n",
      "9/9 [==============================] - 5s 519ms/step - loss: 3.1055\n",
      "Epoch 68/200\n",
      "9/9 [==============================] - 5s 519ms/step - loss: 3.0674\n",
      "Epoch 69/200\n",
      "9/9 [==============================] - 5s 531ms/step - loss: 3.0352\n",
      "Epoch 70/200\n",
      "9/9 [==============================] - 5s 518ms/step - loss: 3.0247\n",
      "Epoch 71/200\n",
      "9/9 [==============================] - 5s 513ms/step - loss: 3.0448\n",
      "Epoch 72/200\n",
      "9/9 [==============================] - 5s 536ms/step - loss: 3.0176\n",
      "Epoch 73/200\n",
      "9/9 [==============================] - 5s 525ms/step - loss: 2.9431\n",
      "Epoch 74/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 2.9395\n",
      "Epoch 75/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 2.9525\n",
      "Epoch 76/200\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 2.9251\n",
      "Epoch 77/200\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 2.8786\n",
      "Epoch 78/200\n",
      "9/9 [==============================] - 5s 511ms/step - loss: 2.8464\n",
      "Epoch 79/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 2.8938\n",
      "Epoch 80/200\n",
      "9/9 [==============================] - 5s 511ms/step - loss: 2.8504\n",
      "Epoch 81/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 2.8459\n",
      "Epoch 82/200\n",
      "9/9 [==============================] - 5s 518ms/step - loss: 2.8049\n",
      "Epoch 83/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 2.7850\n",
      "Epoch 84/200\n",
      "9/9 [==============================] - 5s 512ms/step - loss: 2.7742\n",
      "Epoch 85/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 2.7282\n",
      "Epoch 86/200\n",
      "9/9 [==============================] - 5s 513ms/step - loss: 2.7091\n",
      "Epoch 87/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 2.7155\n",
      "Epoch 88/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 2.7197\n",
      "Epoch 89/200\n",
      "9/9 [==============================] - 5s 512ms/step - loss: 2.6827\n",
      "Epoch 90/200\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 2.6443\n",
      "Epoch 91/200\n",
      "9/9 [==============================] - 5s 514ms/step - loss: 2.6601\n",
      "Epoch 92/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 2.6320\n",
      "Epoch 93/200\n",
      "9/9 [==============================] - 5s 515ms/step - loss: 2.6095\n",
      "Epoch 94/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 2.5838\n",
      "Epoch 95/200\n",
      "9/9 [==============================] - 5s 521ms/step - loss: 2.5805\n",
      "Epoch 96/200\n",
      "9/9 [==============================] - 5s 513ms/step - loss: 2.5489\n",
      "Epoch 97/200\n",
      "9/9 [==============================] - 5s 511ms/step - loss: 2.5535\n",
      "Epoch 98/200\n",
      "9/9 [==============================] - 5s 561ms/step - loss: 2.5196\n",
      "Epoch 99/200\n",
      "9/9 [==============================] - 5s 550ms/step - loss: 2.5184\n",
      "Epoch 100/200\n",
      "9/9 [==============================] - 5s 555ms/step - loss: 2.4550\n",
      "Epoch 101/200\n",
      "9/9 [==============================] - 5s 540ms/step - loss: 2.4617\n",
      "Epoch 102/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 5s 535ms/step - loss: 2.4220\n",
      "Epoch 103/200\n",
      "9/9 [==============================] - 5s 525ms/step - loss: 2.4167\n",
      "Epoch 104/200\n",
      "9/9 [==============================] - 5s 513ms/step - loss: 2.3958\n",
      "Epoch 105/200\n",
      "9/9 [==============================] - 5s 523ms/step - loss: 2.3973\n",
      "Epoch 106/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 2.3757\n",
      "Epoch 107/200\n",
      "9/9 [==============================] - 5s 517ms/step - loss: 2.3319\n",
      "Epoch 108/200\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 2.3384\n",
      "Epoch 109/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 2.3125\n",
      "Epoch 110/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 2.2985\n",
      "Epoch 111/200\n",
      "9/9 [==============================] - 5s 504ms/step - loss: 2.2919\n",
      "Epoch 112/200\n",
      "9/9 [==============================] - 5s 521ms/step - loss: 2.2651\n",
      "Epoch 113/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 2.2206\n",
      "Epoch 114/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 2.2090\n",
      "Epoch 115/200\n",
      "9/9 [==============================] - 5s 502ms/step - loss: 2.2163\n",
      "Epoch 116/200\n",
      "9/9 [==============================] - 5s 512ms/step - loss: 2.1808\n",
      "Epoch 117/200\n",
      "9/9 [==============================] - 5s 505ms/step - loss: 2.1273\n",
      "Epoch 118/200\n",
      "9/9 [==============================] - 5s 518ms/step - loss: 2.1543\n",
      "Epoch 119/200\n",
      "9/9 [==============================] - 5s 516ms/step - loss: 2.1435\n",
      "Epoch 120/200\n",
      "9/9 [==============================] - 5s 512ms/step - loss: 2.1046\n",
      "Epoch 121/200\n",
      "9/9 [==============================] - 5s 521ms/step - loss: 2.0936\n",
      "Epoch 122/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 2.0654\n",
      "Epoch 123/200\n",
      "9/9 [==============================] - 5s 503ms/step - loss: 2.0625\n",
      "Epoch 124/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 2.0527\n",
      "Epoch 125/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 2.0257\n",
      "Epoch 126/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 1.9868\n",
      "Epoch 127/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 1.9778\n",
      "Epoch 128/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 1.9714\n",
      "Epoch 129/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 1.9428\n",
      "Epoch 130/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 1.9555\n",
      "Epoch 131/200\n",
      "9/9 [==============================] - 5s 504ms/step - loss: 1.8765\n",
      "Epoch 132/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 1.9206\n",
      "Epoch 133/200\n",
      "9/9 [==============================] - 5s 515ms/step - loss: 1.8965\n",
      "Epoch 134/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 1.8688\n",
      "Epoch 135/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 1.8583\n",
      "Epoch 136/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 1.8367\n",
      "Epoch 137/200\n",
      "9/9 [==============================] - 5s 502ms/step - loss: 1.8276\n",
      "Epoch 138/200\n",
      "9/9 [==============================] - 5s 505ms/step - loss: 1.8015\n",
      "Epoch 139/200\n",
      "9/9 [==============================] - 5s 534ms/step - loss: 1.8022\n",
      "Epoch 140/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 1.7772\n",
      "Epoch 141/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 1.7684\n",
      "Epoch 142/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 1.7597\n",
      "Epoch 143/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 1.7133\n",
      "Epoch 144/200\n",
      "9/9 [==============================] - 5s 513ms/step - loss: 1.6905\n",
      "Epoch 145/200\n",
      "9/9 [==============================] - 5s 505ms/step - loss: 1.6874\n",
      "Epoch 146/200\n",
      "9/9 [==============================] - 5s 522ms/step - loss: 1.6634\n",
      "Epoch 147/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 1.6599\n",
      "Epoch 148/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 1.6376\n",
      "Epoch 149/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 1.6118\n",
      "Epoch 150/200\n",
      "9/9 [==============================] - 5s 513ms/step - loss: 1.6199\n",
      "Epoch 151/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 1.5885\n",
      "Epoch 152/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 1.5607\n",
      "Epoch 153/200\n",
      "9/9 [==============================] - 5s 517ms/step - loss: 1.5487\n",
      "Epoch 154/200\n",
      "9/9 [==============================] - 5s 502ms/step - loss: 1.5603\n",
      "Epoch 155/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 1.4791\n",
      "Epoch 156/200\n",
      "9/9 [==============================] - 5s 505ms/step - loss: 1.5324\n",
      "Epoch 157/200\n",
      "9/9 [==============================] - 5s 512ms/step - loss: 1.5211\n",
      "Epoch 158/200\n",
      "9/9 [==============================] - 5s 504ms/step - loss: 1.4921\n",
      "Epoch 159/200\n",
      "9/9 [==============================] - 5s 505ms/step - loss: 1.4785\n",
      "Epoch 160/200\n",
      "9/9 [==============================] - 5s 518ms/step - loss: 1.4743\n",
      "Epoch 161/200\n",
      "9/9 [==============================] - 5s 505ms/step - loss: 1.4355\n",
      "Epoch 162/200\n",
      "9/9 [==============================] - 5s 511ms/step - loss: 1.4471\n",
      "Epoch 163/200\n",
      "9/9 [==============================] - 5s 520ms/step - loss: 1.4171\n",
      "Epoch 164/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 1.3951\n",
      "Epoch 165/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 1.3915\n",
      "Epoch 166/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 1.3745\n",
      "Epoch 167/200\n",
      "9/9 [==============================] - 5s 550ms/step - loss: 1.3659\n",
      "Epoch 168/200\n",
      "9/9 [==============================] - 5s 540ms/step - loss: 1.3375\n",
      "Epoch 169/200\n",
      "9/9 [==============================] - 5s 531ms/step - loss: 1.3726\n",
      "Epoch 170/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 1.3108\n",
      "Epoch 171/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 1.3124\n",
      "Epoch 172/200\n",
      "9/9 [==============================] - 5s 517ms/step - loss: 1.2907\n",
      "Epoch 173/200\n",
      "9/9 [==============================] - 5s 506ms/step - loss: 1.2447\n",
      "Epoch 174/200\n",
      "9/9 [==============================] - 5s 514ms/step - loss: 1.2320\n",
      "Epoch 175/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 1.2620\n",
      "Epoch 176/200\n",
      "9/9 [==============================] - 5s 522ms/step - loss: 1.2379\n",
      "Epoch 177/200\n",
      "9/9 [==============================] - 5s 523ms/step - loss: 1.2045\n",
      "Epoch 178/200\n",
      "9/9 [==============================] - 5s 521ms/step - loss: 1.2044\n",
      "Epoch 179/200\n",
      "9/9 [==============================] - 5s 561ms/step - loss: 1.2024\n",
      "Epoch 180/200\n",
      "9/9 [==============================] - 5s 577ms/step - loss: 1.2013\n",
      "Epoch 181/200\n",
      "9/9 [==============================] - 5s 519ms/step - loss: 1.1629\n",
      "Epoch 182/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 1.1321\n",
      "Epoch 183/200\n",
      "9/9 [==============================] - 5s 534ms/step - loss: 1.1461\n",
      "Epoch 184/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 1.1183\n",
      "Epoch 185/200\n",
      "9/9 [==============================] - 5s 513ms/step - loss: 1.1261\n",
      "Epoch 186/200\n",
      "9/9 [==============================] - 5s 505ms/step - loss: 1.1072\n",
      "Epoch 187/200\n",
      "9/9 [==============================] - 5s 516ms/step - loss: 1.0974\n",
      "Epoch 188/200\n",
      "9/9 [==============================] - 5s 503ms/step - loss: 1.0703\n",
      "Epoch 189/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 1.0827\n",
      "Epoch 190/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 1.0560\n",
      "Epoch 191/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 1.0348\n",
      "Epoch 192/200\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 1.0182\n",
      "Epoch 193/200\n",
      "9/9 [==============================] - 5s 508ms/step - loss: 1.0418\n",
      "Epoch 194/200\n",
      "9/9 [==============================] - 5s 518ms/step - loss: 1.0056\n",
      "Epoch 195/200\n",
      "9/9 [==============================] - 6s 628ms/step - loss: 0.9869\n",
      "Epoch 196/200\n",
      "9/9 [==============================] - 5s 507ms/step - loss: 1.0013\n",
      "Epoch 197/200\n",
      "9/9 [==============================] - 4s 496ms/step - loss: 0.9648\n",
      "Epoch 198/200\n",
      "9/9 [==============================] - 4s 493ms/step - loss: 0.9563\n",
      "Epoch 199/200\n",
      "9/9 [==============================] - 4s 494ms/step - loss: 0.9535\n",
      "Epoch 200/200\n",
      "9/9 [==============================] - 4s 493ms/step - loss: 0.9354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1eb907da9d0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=32, epochs=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f840cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=(200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=(200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a9b21055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(input_sentence):\n",
    "    tokens = input_sentence.lower().split()\n",
    "    tokens_list = []\n",
    "    for word in tokens:\n",
    "        tokens_list.append(tokenizer.word_index[word]) \n",
    "    return preprocessing.sequence.pad_sequences([tokens_list] , maxlen=maxlen_questions , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5d7439b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model , dec_model = inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e30b6f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "820"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['robô']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "794b76ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Human: oi\n",
      "\n",
      "Bot:  olá\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Human: Você é um robô\n",
      "\n",
      "Bot:  você é que é\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Human: o que é linguística\n",
      "\n",
      "Bot:  bom a alfabetização é a ciência que estuda os fenômenos relacionados e o nome em um nome é a menos que não seja holandês agora\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Human: o que é um opinião\n",
      "\n",
      "Bot:  bem a alfabetização pode dar fazer à margem da instituição escolar e o nome em um nome em um livro cujo nome\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Human: até mais\n",
      "\n",
      "Bot:  até\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "tests = ['oi', 'Você é um robô', 'o que é linguística', 'o que é um opinião', 'até mais']\n",
    "\n",
    "for i, frase in enumerate(tests):\n",
    "    states_values = enc_model.predict(preprocess_input(tests[i]))\n",
    "    empty_target_seq = np.zeros((1 , 1))\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    \n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
    "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "        sampled_word = None\n",
    "        \n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += f' {word}'\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros((1 , 1))  \n",
    "        empty_target_seq[0 , 0] = sampled_word_index\n",
    "        states_values = [h , c] \n",
    "    print(f'Human: {tests[i]}')\n",
    "    print()\n",
    "    decoded_translation = decoded_translation.split(' end')[0]\n",
    "    print(f'Bot: {decoded_translation}')\n",
    "    print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07abc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
